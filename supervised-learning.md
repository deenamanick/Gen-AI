Here are some practical exercises for supervised learning, ranging from beginner to intermediate levels. These exercises will help you understand different algorithms, data preprocessing, model evaluation, and more.

---

### **1. Beginner Exercises**
#### **A. Linear Regression**
**Problem:** Predict house prices based on features like area, number of bedrooms, and location.  
**Dataset:** [Boston Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html) (or California Housing Dataset).  
**Tasks:**
1. Load and explore the dataset (check for missing values, distributions, correlations).
2. Split the data into training and testing sets.
3. Train a **Linear Regression** model.
4. Evaluate using **Mean Squared Error (MSE)** and **RÂ² Score**.
5. Try **feature scaling** (StandardScaler) and see if it improves performance.
6. (Optional) Compare with **Ridge/Lasso Regression**.

---

#### **B. Logistic Regression (Classification)**
**Problem:** Predict whether a patient has diabetes.  
**Dataset:** [Pima Indians Diabetes Dataset](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database).  
**Tasks:**
1. Perform **exploratory data analysis (EDA)** (check class imbalance, missing values).
2. Split data into train-test sets.
3. Train a **Logistic Regression** model.
4. Evaluate using **accuracy, precision, recall, F1-score, and ROC-AUC**.
5. Try **feature scaling** and see if it improves performance.
6. (Optional) Use **SMOTE** for handling class imbalance.

---

### **2. Intermediate Exercises**
#### **A. Decision Trees & Random Forest**
**Problem:** Predict whether a passenger survived the Titanic disaster.  
**Dataset:** [Titanic Dataset](https://www.kaggle.com/c/titanic).  
**Tasks:**
1. Perform **EDA** and **feature engineering** (handle missing values, encode categorical variables).
2. Split data into train-test sets.
3. Train a **Decision Tree** and tune hyperparameters (`max_depth`, `min_samples_split`).
4. Train a **Random Forest** and compare performance.
5. Evaluate using **accuracy, precision, recall, and confusion matrix**.
6. (Optional) Use **GridSearchCV** for hyperparameter tuning.

---

#### **B. Support Vector Machines (SVM)**
**Problem:** Classify handwritten digits (0-9).  
**Dataset:** [MNIST Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) (smaller version).  
**Tasks:**
1. Explore the dataset (visualize some digits).
2. Split into train-test sets.
3. Train an **SVM with a linear kernel**.
4. Train an **SVM with an RBF kernel** and tune `C` and `gamma`.
5. Evaluate using **classification report and confusion matrix**.
6. (Optional) Compare with **K-Nearest Neighbors (KNN)**.

---

### **3. Advanced Exercises**
#### **A. Gradient Boosting (XGBoost/LightGBM)**
**Problem:** Predict credit card fraud (highly imbalanced dataset).  
**Dataset:** [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud).  
**Tasks:**
1. Perform EDA (check class imbalance, feature distributions).
2. Split data into train-test sets.
3. Train an **XGBoost** or **LightGBM** model.
4. Handle imbalance using **class_weight or SMOTE**.
5. Evaluate using **precision-recall curve, F1-score, and ROC-AUC**.
6. (Optional) Use **SHAP values** for feature importance.

---

#### **B. Neural Networks (Basic MLP)**
**Problem:** Predict bike rental demand (regression).  
**Dataset:** [Bike Sharing Dataset](https://www.kaggle.com/datasets/lakshmi25npathi/bike-sharing-dataset).  
**Tasks:**
1. Perform feature engineering (handle datetime, categorical variables).
2. Scale features using `StandardScaler` or `MinMaxScaler`.
3. Train a simple **Multi-Layer Perceptron (MLP)** using Keras/TensorFlow.
4. Evaluate using **MSE and MAE**.
5. (Optional) Compare with **Random Forest Regression**.

---
